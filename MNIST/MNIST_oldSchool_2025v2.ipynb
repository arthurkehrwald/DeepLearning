{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lf7Ciw1ILUJM"
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    x = torch.ones(1, device=device)\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "# Umschalten zwischen Colab oder lokaler Installation\n",
    "USING_COLAB = False\n",
    "if USING_COLAB:\n",
    "    from google.colab import drive\n",
    "    from google.colab.patches import cv2_imshow\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQXCN16HLUJR"
   },
   "source": [
    "Download and load the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bp6nmcGfLUJW"
   },
   "outputs": [],
   "source": [
    "def get_data(batch_size: int = 32) -> Tuple[DataLoader, DataLoader]:\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            # Converts to float and normalizes from [0, 255] to [0, 1]\n",
    "            transforms.ToTensor(),\n",
    "            # Flattens the 2D image 28x28 to 1D vector 784\n",
    "            transforms.Lambda(lambda x: x.view(-1)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_set = datasets.MNIST(\"data/\", download=True, train=True, transform=transform)\n",
    "    test_set = datasets.MNIST(\"data/\", download=True, train=False, transform=transform)\n",
    "\n",
    "    train_dl = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_dl = DataLoader(test_set, batch_size=len(test_set), shuffle=False)\n",
    "\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dCqz5klinHn"
   },
   "source": [
    "Modell (KNN) definieren mit beliebig vielen Schichten, die jeweils variable Anzahl Neuronen beinhalten. Wir beginnen hier immer mit 28x28 Eingabe-Neuronen und müssen am Ende immer auf 10 Ausgabe-Neuronen kommen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynAnfCnvLUJX"
   },
   "outputs": [],
   "source": [
    "def get_model() -> tuple[nn.Module, nn.Module, SGD]:\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(28 * 28, 50),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(50, 40),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(40, 30),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(30, 20),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(20, 10),\n",
    "    ).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "    return model, loss_fn, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxyRBE2qYL45"
   },
   "outputs": [],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    if type(m) == nn.Linear:\n",
    "        # m.weight.data.fill_(1)\n",
    "        # m.weight.data.uniform_(-0.1, 0.1)\n",
    "        m.weight.data.normal_(0.0, 0.1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDA7bVhtLUJY"
   },
   "outputs": [],
   "source": [
    "def train_batch(\n",
    "    x: torch.Tensor, y: torch.Tensor, model: nn.Module, opt: SGD, loss_fn: nn.Module\n",
    ") -> float:\n",
    "    model.train()\n",
    "\n",
    "    prediction = model(x)\n",
    "    # print(f\"prediction.shape {prediction.shape}\")\n",
    "\n",
    "    batch_loss = loss_fn(prediction, y)\n",
    "    # print(f\"batch_loss {batch_loss}\")\n",
    "\n",
    "    batch_loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    return batch_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQ2KVHOOLUJZ"
   },
   "outputs": [],
   "source": [
    "def accuracy(x: torch.Tensor, y: torch.Tensor, model: nn.Module):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(x)\n",
    "\n",
    "    max_values, argmaxes = prediction.max(-1)\n",
    "    is_correct = argmaxes == y\n",
    "\n",
    "    return is_correct.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5Efay_nLUJa"
   },
   "outputs": [],
   "source": [
    "def loss(x: torch.Tensor, y: torch.Tensor, model: nn.Module, loss_fn: nn.Module):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(x)\n",
    "        loss = loss_fn(prediction, y)\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_F7QVcLUJb"
   },
   "outputs": [],
   "source": [
    "train_dl, test_dl = get_data()\n",
    "model, loss_fn, optimizer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sOWl018LUJc"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Training >>>\n",
    "#\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Hier werden die initialen Gewichte des Netzes zufällig gesetzt\n",
    "# model.apply(init_weights)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "arrPlotX = []\n",
    "train_losses, train_accuracies = [], []\n",
    "test_losses, test_accuracies = [], []\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    timeBeginEpoch = timer()\n",
    "    train_epoch_losses, train_epoch_accuracies = [], []\n",
    "\n",
    "    for ix, batch in enumerate(iter(train_dl)):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # print(f\"x.shape: {x.shape}  y.shape: {y.shape}\\n\")\n",
    "\n",
    "        batch_loss = train_batch(x, y, model, optimizer, loss_fn)\n",
    "        train_epoch_losses.append(batch_loss)\n",
    "        is_correct = accuracy(x, y, model)\n",
    "        train_epoch_accuracies.extend(is_correct)\n",
    "\n",
    "    train_epoch_loss = np.array(train_epoch_losses).mean()\n",
    "    train_epoch_accuracy = np.mean(train_epoch_accuracies)\n",
    "\n",
    "    for ix, batch in enumerate(iter(test_dl)):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        val_is_correct = accuracy(x, y, model)\n",
    "        validation_loss = loss(x, y, model, loss_fn)\n",
    "\n",
    "    val_epoch_accuracy = np.mean(val_is_correct)\n",
    "    arrPlotX.append(epoch)\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    train_accuracies.append(train_epoch_accuracy)\n",
    "    test_losses.append(validation_loss)\n",
    "    test_accuracies.append(val_epoch_accuracy)\n",
    "    timeEndEpoch = timer()\n",
    "    print(\n",
    "        f\"epoch: {epoch}, train_acc: {100 * train_epoch_accuracy:.2f}%, test_acc: {100 * val_epoch_accuracy:.2f}%, took {timeEndEpoch-timeBeginEpoch:.1f}s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuZmxWzqLUJe"
   },
   "outputs": [],
   "source": [
    "if USING_COLAB:\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        \"/content/drive/My Drive/ColabNotebooks/results/nnMnist_exp01.pt\",\n",
    "    )\n",
    "else:\n",
    "    torch.save(model.state_dict(), \"nnMnist_exp01.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDkUvtl_LUJe"
   },
   "outputs": [],
   "source": [
    "plt.plot(arrPlotX, train_accuracies)\n",
    "plt.plot(arrPlotX, test_accuracies)\n",
    "plt.title(\"Accuracy vs. Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Train\", \"Test\"])\n",
    "\n",
    "if USING_COLAB:\n",
    "    plt.savefig(\"/content/drive/My Drive/ColabNotebooks/results/accuracies_exp0.png\")\n",
    "else:\n",
    "    plt.savefig(\"accuracies_exp0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FR9XI_13y-0D"
   },
   "outputs": [],
   "source": [
    "plt.plot(arrPlotX, train_losses)\n",
    "plt.plot(arrPlotX, test_losses)\n",
    "plt.title(\"Loss vs. Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Train\", \"Test\"])\n",
    "\n",
    "if USING_COLAB:\n",
    "    plt.savefig(\"/content/drive/My Drive/ColabNotebooks/results/losses_exp0.png\")\n",
    "else:\n",
    "    plt.savefig(\"losses_exp0.png\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "biomasse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
