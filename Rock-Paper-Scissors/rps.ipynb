{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d01c3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple, Callable\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "from enum import Enum\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "471ff0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mode(Enum):\n",
    "    TRAIN = 0\n",
    "    LOAD = 1\n",
    "\n",
    "class Classes(Enum):\n",
    "    PAPER = 0\n",
    "    OTHER = 1\n",
    "    ROCK = 2\n",
    "    SCISSORS = 3\n",
    "\n",
    "MODE = Mode.LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "601de4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    x = torch.ones(1, device=device)\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20581d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(\n",
    "    batch_size: int, resolution: Tuple[int, int]\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    transform = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Resize(resolution),  # 3:2 aspect ratio\n",
    "            torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    data_dir = \"data\"\n",
    "    dataset = torchvision.datasets.ImageFolder(data_dir, transform)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "919e4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_labels(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    batch_lim: int = 0,\n",
    "    silent: bool = False,\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for i, (data, labels) in tqdm(enumerate(iter(loader)), disable=silent):\n",
    "\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "\n",
    "        _, predictions = torch.max(output, 1)\n",
    "        # Stack predictions and labels for each sample in the batch\n",
    "        batch_results = torch.stack((predictions, labels), dim=1).detach().cpu()\n",
    "        if i == 0:\n",
    "            results = batch_results\n",
    "        else:\n",
    "            results = torch.cat((results, batch_results), dim=0)\n",
    "        if batch_lim > 0 and i >= batch_lim:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_correct_percent(results: torch.Tensor) -> float:\n",
    "    # results is a tensor of shape (N, 2): [prediction, label]\n",
    "    correct = (results[:, 0] == results[:, 1]).sum().item()\n",
    "    return correct / len(results) * 100\n",
    "\n",
    "\n",
    "def save_miscategorized_images(\n",
    "    results: torch.Tensor,\n",
    "    val_loader: DataLoader,\n",
    "    path: str,\n",
    "    max_img_count: int = 20,\n",
    "):\n",
    "    # Create directory for misclassified images\n",
    "    # results is a tensor of shape (N, 2): [prediction, label]\n",
    "    misclassified_mask = results[:, 0] != results[:, 1]\n",
    "    misclassified_indices = misclassified_mask.nonzero(as_tuple=True)[0][:max_img_count]\n",
    "\n",
    "    misclassified_dir = os.path.join(path, \"misclassified\")\n",
    "    os.makedirs(misclassified_dir, exist_ok=True)\n",
    "\n",
    "    # Get the dataset from the DataLoader\n",
    "    dataset = val_loader.dataset\n",
    "\n",
    "    # If val_loader is a Subset, get the original indices\n",
    "    if hasattr(dataset, \"indices\"):\n",
    "        indices_map = dataset.indices\n",
    "    else:\n",
    "        indices_map = range(len(dataset))\n",
    "\n",
    "    for idx, mis_idx in enumerate(misclassified_indices):\n",
    "        orig_idx = indices_map[mis_idx.item()]\n",
    "        img_path, label = dataset.dataset.samples[orig_idx]\n",
    "        pred = int(results[mis_idx, 0].item())\n",
    "        actual = int(results[mis_idx, 1].item())\n",
    "        dst = os.path.join(\n",
    "            misclassified_dir,\n",
    "            f\"misclassified_{idx}_pred{pred}_actual{actual}_{os.path.basename(img_path)}\"\n",
    "        )\n",
    "        shutil.copy(img_path, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e135cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    epochs: int,\n",
    ") -> None:\n",
    "    for epoch in range(epochs):\n",
    "        progress_bar = tqdm(\n",
    "            enumerate(iter(train_loader)),\n",
    "            total=len(train_loader),\n",
    "            desc=f\"Epoch {epoch+1}/{epochs}\",\n",
    "        )\n",
    "        model.train()\n",
    "        for batchIdx, (data, labels) in progress_bar:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(data)\n",
    "            batch_loss = loss_fn(predictions, labels)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            if batchIdx == 0:\n",
    "                batch_losses = batch_loss.detach().unsqueeze(0)\n",
    "            else:\n",
    "                batch_losses = torch.cat(\n",
    "                    (batch_losses, batch_loss.detach().unsqueeze(0))\n",
    "                )\n",
    "            progress_bar.set_postfix(\n",
    "                {\"Mean batch loss\": torch.mean(batch_losses).item()}\n",
    "            )\n",
    "        val_acc = get_correct_percent(\n",
    "            get_predictions_and_labels(model, val_loader, device, batch_lim=1, silent=True)\n",
    "        )\n",
    "        train_acc = get_correct_percent(\n",
    "            get_predictions_and_labels(model, train_loader, device, batch_lim=1, silent=True)\n",
    "        )\n",
    "        print(f\"Val Acc: {val_acc:.2f}% Train Acc: {train_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86c5bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, loader, device) -> float:\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = net(data)\n",
    "            _, prediction = torch.max(output, 1)\n",
    "            correct += torch.sum(prediction == target).item()\n",
    "            total += target.size(0)\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df63397b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "resolution = (150, 100)\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "train_loader, val_loader = get_data(batch_size, resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26958770",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Net:\n\tMissing key(s) in state_dict: \"conv_layers.12.weight\", \"conv_layers.12.bias\", \"conv_layers.15.weight\", \"conv_layers.15.bias\", \"fc_layers.5.weight\", \"fc_layers.5.bias\". \n\tsize mismatch for fc_layers.2.weight: copying a param with shape torch.Size([4, 128]) from checkpoint, the shape in current model is torch.Size([64, 512]).\n\tsize mismatch for fc_layers.2.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m MODE == Mode.LOAD:\n\u001b[32m      4\u001b[39m     model = Net()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDavidsRockPaperNet.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.to(\n\u001b[32m      6\u001b[39m         device\n\u001b[32m      7\u001b[39m     )\n\u001b[32m      8\u001b[39m     summary(model, (\u001b[32m1\u001b[39m, *resolution), batch_size, device.type)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m MODE == Mode.TRAIN:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\repos\\learn\\MAMT-DLO\\Rock-Paper-Scissors\\.rockpaperscissorsvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for Net:\n\tMissing key(s) in state_dict: \"conv_layers.12.weight\", \"conv_layers.12.bias\", \"conv_layers.15.weight\", \"conv_layers.15.bias\", \"fc_layers.5.weight\", \"fc_layers.5.bias\". \n\tsize mismatch for fc_layers.2.weight: copying a param with shape torch.Size([4, 128]) from checkpoint, the shape in current model is torch.Size([64, 512]).\n\tsize mismatch for fc_layers.2.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([64])."
     ]
    }
   ],
   "source": [
    "MODEL_FILE_NAME = \"rps2.pt\"\n",
    "\n",
    "if MODE == Mode.LOAD:\n",
    "    model = torch.load(MODEL_FILE_NAME, weights_only=False).to(device)\n",
    "    summary(model, (1, *resolution), batch_size, device.type)\n",
    "elif MODE == Mode.TRAIN:\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 6, 5),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        nn.Conv2d(6, 16, 5),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(11968, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 84),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(84, 4),\n",
    "    ).to(device)\n",
    "    summary(model, (1, *resolution), batch_size, device.type)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d32ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [256, 6, 146, 96]             156\n",
      "              ReLU-2          [256, 6, 146, 96]               0\n",
      "         MaxPool2d-3           [256, 6, 73, 48]               0\n",
      "            Conv2d-4          [256, 16, 69, 44]           2,416\n",
      "              ReLU-5          [256, 16, 69, 44]               0\n",
      "         MaxPool2d-6          [256, 16, 34, 22]               0\n",
      "           Flatten-7               [256, 11968]               0\n",
      "            Linear-8                 [256, 128]       1,532,032\n",
      "              ReLU-9                 [256, 128]               0\n",
      "           Linear-10                  [256, 84]          10,836\n",
      "             ReLU-11                  [256, 84]               0\n",
      "           Linear-12                   [256, 4]             340\n",
      "================================================================\n",
      "Total params: 1,545,780\n",
      "Trainable params: 1,545,780\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 14.65\n",
      "Forward/backward pass size (MB): 606.90\n",
      "Params size (MB): 5.90\n",
      "Estimated Total Size (MB): 627.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, *resolution), batch_size, device.type)\n",
    "\n",
    "\n",
    "if MODE == Mode.TRAIN:\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.03)\n",
    "    train(model, optimizer, loss_fn, train_loader, val_loader, device, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef20b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:04,  2.29s/it]\n",
      "5it [00:16,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 85.33%, Val Acc: 83.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = get_predictions_and_labels(model, val_loader, device)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_artifacts_dir = os.path.join(\"run_artifacts\", timestamp)\n",
    "os.makedirs(run_artifacts_dir, exist_ok=True)\n",
    "save_miscategorized_images(results, val_loader, run_artifacts_dir, 100)\n",
    "val_acc = get_correct_percent(results)\n",
    "train_acc = get_correct_percent(get_predictions_and_labels(model, train_loader, device))\n",
    "print(f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb434e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(run_artifacts_dir, MODEL_FILE_NAME)\n",
    "torch.save(model, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rockpaperscissorsvenv",
   "language": "python",
   "name": ".rockpaperscissorsvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
